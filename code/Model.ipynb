{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jin Zhang\n",
    "# Use Python 2 \n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams.update({'font.size': 22})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline, preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict,cross_val_score, StratifiedShuffleSplit,GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, \\\n",
    "                            accuracy_score, f1_score, roc_auc_score, roc_curve, \\\n",
    "                             precision_recall_curve,log_loss, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from enchant.checker import SpellChecker\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "lme4=importr('lme4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_use = pd.read_csv('./input/df_use.csv', parse_dates= ['date_posted', 'date_completed','date_thank_you_packet_mailed', 'date_expiration'])\n",
    "df_outsample = pd.read_csv('./input/df_outsample.csv', parse_dates= ['date_posted', 'date_completed','date_thank_you_packet_mailed', 'date_expiration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_use = df_use.rename(columns = {'item_total_expense':'most_exp_item_cost','item_expense':'project_expense'})\n",
    "\n",
    "df_use.loc[pd.isnull(df_use.most_exp_item_cost),'most_exp_item_cost'] = \\\n",
    "                              df_use.loc[pd.isnull(df_use.most_exp_item_cost),'project_expense'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_outsample = df_outsample.rename(columns = {'item_total_expense':'most_exp_item_cost',\\\n",
    "                                              'item_expense':'project_expense'})\n",
    "\n",
    "df_outsample.loc[pd.isnull(df_outsample.most_exp_item_cost),'most_exp_item_cost'] = \\\n",
    "                              df_outsample.loc[pd.isnull(df_outsample.most_exp_item_cost),'project_expense'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_outsample = df_outsample[df_outsample.funding_status!='live']\n",
    "df_outsample = df_outsample[~pd.isnull(df_outsample.most_exp_item_cost)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_outsample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all, y_all, X_outsample, y_outsample = df_use.drop('funding_success',axis=1), df_use['funding_success'],\\\n",
    "                                         df_outsample.drop('funding_success',axis=1), df_outsample['funding_success']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs0 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=100)\n",
    "\n",
    "for modeling_index,holdout_index in rs0.split(X_all,y_all):\n",
    "    X = X_all.iloc[modeling_index,:]\n",
    "    X_holdout =  X_all.iloc[holdout_index,:]\n",
    "    y = y_all.iloc[modeling_index]\n",
    "    y_holdout = y_all.iloc[holdout_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=102)\n",
    "\n",
    "for train_index, test_index in rs.split(X,y):\n",
    "    X_train = X.iloc[train_index,:]\n",
    "    X_test =  X.iloc[test_index,:]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define pipeline classes and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge dictionary helper function\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Column_Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,selected_columns, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.selected_columns=selected_columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.selected_columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Get_Location_Input(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.model = None\n",
    "    def fit(self, X, y=None):\n",
    "        steps = [('test',Column_Selector(selected_columns=['school_latitude','school_longitude'])),\n",
    "        ('kn',KNeighborsClassifier(n_neighbors  = 184, weights='distance')),]\n",
    "        kn1 = pipeline.Pipeline(steps)\n",
    "        kn1.fit(X,y)\n",
    "        self.model = kn1\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return pd.DataFrame(pd.Series(self.model.predict(X), index = X.index, name='location'))\n",
    "    def _get_param_names(self):\n",
    "        return ['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tune parameter for knearest neighbors\n",
    "param_kn1 = dict(kn__n_neighbors = range(180,189,2))\n",
    "\n",
    "steps = [('test',Column_Selector(selected_columns=['school_latitude','school_longitude'])),\n",
    "        ('kn',KNeighborsClassifier(weights='distance')),]\n",
    "kn1 = pipeline.Pipeline(steps)\n",
    "\n",
    "grid_kn1= GridSearchCV(kn1,\\\n",
    "                    param_grid=param_kn1, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_kn1.fit(X_train,y_train)\n",
    "#grid1.cv_results_, \\\n",
    "grid_kn1.best_params_, \\\n",
    "grid_kn1.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Get_Raw_Features(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X=X.assign(project_duration = (X['date_expiration'] - X['date_posted'])/np.timedelta64(1, 'D'),\\\n",
    "           has_secondary_focus = X['secondary_focus_subject'].isnull().astype(int))\n",
    "\n",
    "\n",
    "        d_boolean = {'t':1,'f':0}\n",
    "        X_boolean =  X[['school_charter','school_magnet','school_year_round',\\\n",
    "                        'school_nlns','school_kipp','school_charter_ready_promise',\\\n",
    "                        'teacher_teach_for_america','teacher_ny_teaching_fellow',\\\n",
    "                        'eligible_double_your_impact_match','eligible_almost_home_match']]\\\n",
    "                              .applymap(lambda x: d_boolean[x])\n",
    "\n",
    "        X_numeric = X[['has_secondary_focus','students_reached','posted_month','most_exp_item_cost',\\\n",
    "                       'project_expense','total_price_excluding_optional_support','project_duration']]\n",
    "        \n",
    "        \n",
    "        return pd.concat([X_boolean,X_numeric],axis=1)\n",
    "    def _get_param_names(self):\n",
    "        return (['school_charter','school_magnet','school_year_round',\\\n",
    "                        'school_nlns','school_kipp','school_charter_ready_promise',\\\n",
    "                        'teacher_teach_for_america','teacher_ny_teaching_fellow',\\\n",
    "                        'eligible_double_your_impact_match','eligible_almost_home_match'] +\n",
    "               ['has_secondary_focus','students_reached','posted_month','most_exp_item_cost',\\\n",
    "                       'project_expense','total_price_excluding_optional_support','project_duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Get_Category_Mean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.stored_result = {}\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X.teacher_prefix = X.teacher_prefix.fillna('N/A')\n",
    "        X.grade_level = X.grade_level.fillna('N/A')\n",
    "        \n",
    "        #y_data = pd.Series(y, name ='funding_success',index=X.index)\n",
    "        df = pd.concat([X,y],axis=1)\n",
    "        \n",
    "        category_var_list = ['teacher_prefix','primary_focus_subject','primary_focus_area',\\\n",
    "                     'resource_type','poverty_level','grade_level']        \n",
    "        z = {}\n",
    "        for var in category_var_list:\n",
    "            \n",
    "            z = merge_two_dicts(z, df.groupby(var,as_index=False)[['funding_success']]\\\n",
    "                               .mean().rename(columns={'funding_success':var+'_mean'}).to_dict())\n",
    "            \n",
    "        self.stored_result = z\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.teacher_prefix = X.teacher_prefix.fillna('N/A')\n",
    "        X.grade_level = X.grade_level.fillna('N/A')\n",
    "        \n",
    "        X_merge = X.copy(deep=True)\n",
    "        category_var_list = ['teacher_prefix','primary_focus_subject','primary_focus_area',\\\n",
    "                             'resource_type','poverty_level','grade_level'] \n",
    "        \n",
    "        for var in category_var_list: \n",
    "            join_d = {}\n",
    "            for index in [var,var+'_mean']:\n",
    "                join_d[index] = self.stored_result[index]\n",
    "            \n",
    "            X_merge = X_merge.merge(pd.DataFrame.from_dict(join_d), how='left', on = var)\n",
    "            \n",
    "        return X_merge.filter(regex='_mean')\n",
    "        \n",
    "    def _get_param_names(self):\n",
    "        param_list = []\n",
    "        category_var_list = ['teacher_prefix','primary_focus_subject','primary_focus_area',\\\n",
    "                     'resource_type','poverty_level','grade_level']\n",
    "        for var in category_var_list:\n",
    "            param_list.append(var+'_mean')\n",
    "        return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Get_Category_ShrinkMean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.stored_result = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X.teacher_prefix = X.teacher_prefix.fillna('N/A')\n",
    "        X.grade_level = X.grade_level.fillna('N/A')\n",
    "        \n",
    "        category_var_list = ['teacher_prefix','primary_focus_subject','primary_focus_area',\\\n",
    "                     'resource_type','poverty_level','grade_level']\n",
    "        z = {}\n",
    "        for var in category_var_list:\n",
    "            rX = pandas2ri.py2ri(X[[var]].reset_index(drop=True))\n",
    "            ry = pandas2ri.py2ri(y.reset_index(drop=True))\n",
    "            \n",
    "            rstring=\"\"\"\n",
    "            function(rX,ry){\n",
    "                library(lme4)\n",
    "                                \n",
    "                fid <- factor(rX[,1])\n",
    "                mod <-lmer(ry ~ 1  + (1|fid))            \n",
    "\n",
    "                df <- data.frame(ranef(mod)$fid+fixef(mod))\n",
    "                df\n",
    "\n",
    "            }\n",
    "            \"\"\"\n",
    "            rfunc=robjects.r(rstring)\n",
    "            r_df=rfunc(rX,ry)\n",
    "            pred=pandas2ri.ri2py(r_df).reset_index()\n",
    "            pred.columns = [var,var+'_shrink_mean']\n",
    "            \n",
    "            \n",
    "            z = merge_two_dicts(z, pred.to_dict())\n",
    "            \n",
    "        self.stored_result = z\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def transform(self, X):\n",
    "        X.teacher_prefix = X.teacher_prefix.fillna('N/A')\n",
    "        X.grade_level = X.grade_level.fillna('N/A')\n",
    "        \n",
    "        X_merge = X.copy(deep=True)\n",
    "        category_var_list = ['teacher_prefix','primary_focus_subject','primary_focus_area',\\\n",
    "                             'resource_type','poverty_level','grade_level'] \n",
    "        \n",
    "        for var in category_var_list: \n",
    "            join_d = {}\n",
    "            for index in [var,var+'_shrink_mean']:\n",
    "                join_d[index] = self.stored_result[index]\n",
    "            \n",
    "            X_merge = X_merge.merge(pd.DataFrame.from_dict(join_d), how='left', on = var)\n",
    "            \n",
    "        return X_merge.filter(regex='_shrink_mean')\n",
    "        \n",
    "    def _get_param_names(self):\n",
    "        param_list = []\n",
    "        category_var_list = ['teacher_prefix','primary_focus_subject','primary_focus_area',\\\n",
    "                     'resource_type','poverty_level','grade_level']\n",
    "        for var in category_var_list:\n",
    "            param_list.append(var+'_shrink_mean')\n",
    "        return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Get_Text_Features(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text_var_list = ['title','need_statement','essay']\n",
    "        X_text_basic = pd.DataFrame({})\n",
    "        for var in text_var_list:\n",
    "            X_text_basic = pd.concat([X_text_basic, \n",
    "                           X[[var]].apply(lambda x: x.str.split().str.len()).rename(columns = {var: var +'_word_count'})],\\\n",
    "                           axis=1)\n",
    "            X_text_basic = pd.concat([X_text_basic,\n",
    "                           X[[var]].apply(lambda x: x.str.count('!')).rename(columns = {var: var +'_exclamation_count'})],\\\n",
    "                           axis=1)\n",
    "            X_text_basic = pd.concat([X_text_basic, \n",
    "                           X[[var]].apply(lambda x: x.str.count(r'\\?')).rename(columns = {var: var +'_question_count'})],\\\n",
    "                           axis=1)\n",
    "        return X_text_basic\n",
    "    def _get_param_names(self):\n",
    "        param_list = []\n",
    "        text_var_list = ['title','need_statement','essay']\n",
    "        for var in text_var_list:\n",
    "            param_list.append(var +'_word_count')\n",
    "            param_list.append(var +'_exclamation_count')\n",
    "            param_list.append(var +'_question_count')\n",
    "        return param_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_feature_importances(model,feature_union_step,index_list=[]):\n",
    "    if 'feature_importances_' in dir(model):\n",
    "        coefs = list(model.feature_importances_)\n",
    "    elif 'coef_' in dir(model):\n",
    "        coefs = list(model.coef_[0])\n",
    "    if coefs:\n",
    "        feature_names = []\n",
    "        for item in reg2.named_steps['combined'].transformer_list:\n",
    "            feature_names += item[1]._get_param_names()\n",
    "        if index_list:\n",
    "            result = []\n",
    "            for index,item in enumerate(feature_names):\n",
    "                if index in index_list:\n",
    "                    result.append(item)\n",
    "            feature_names = result\n",
    "        features = sorted(zip(feature_names,coefs),key=lambda x: abs(x[1]),reverse=True)\n",
    "        for f in features:\n",
    "            print(\"{}: {}\".format(f[0],f[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred_dummy = np.ones_like(y_test)\n",
    "y_holdout_pred_dummy = np.ones_like(y_holdout)\n",
    "y_outsample_pred_dummy = np.ones_like(y_outsample)\n",
    "\n",
    "\n",
    "roc_auc_score(y_test,y_test_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score(y_test, y_test_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regular features with categorical mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_model_result(model, model_name,feature,feature_name,X,y):\n",
    "\n",
    "    \n",
    "    steps = [(feature_name,feature),\n",
    "             (model_name,model),]\n",
    "\n",
    "    reg = pipeline.Pipeline(steps)\n",
    "    reg.fit(X,y)\n",
    "    \n",
    "    print(model_name + ' test set has auc score:', np.mean(cross_val_score(reg, X, y, scoring = 'roc_auc')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dict = {'logistic regression': LogisticRegression(),\n",
    "              'gaussian naive bayes': GaussianNB(),\n",
    "              'decision tree classifier': DecisionTreeClassifier(),\n",
    "              'random forest classifier': RandomForestClassifier(class_weight='balanced_subsample'),\n",
    "              'gradient boosting classifier': GradientBoostingClassifier()\n",
    "             }\n",
    "\n",
    "feature_name = 'combined'\n",
    "combined_features = pipeline.FeatureUnion([('raw_features', Get_Raw_Features()),\n",
    "                                               ('category_mean',Get_Category_Mean()),\n",
    "                                               ('text_features',Get_Text_Features()),\n",
    "                                               ('location_feature',Get_Location_Input()),\n",
    "                                              ])\n",
    "for model_name, model in model_dict.items():\n",
    "    print_model_result(model, model_name,combined_features,feature_name,X,y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features with shrinkage mean\n",
    "# performs on par with categorical mean, therefore not use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dict = {'logistic regression': LogisticRegression(),\n",
    "              'gaussian naive bayes': GaussianNB(),\n",
    "              'decision tree classifier': DecisionTreeClassifier(),\n",
    "              'random forest classifier': RandomForestClassifier(class_weight='balanced_subsample'),\n",
    "              'gradient boosting classifier': GradientBoostingClassifier()\n",
    "             }\n",
    "\n",
    "feature_name = 'combined'\n",
    "combined_features_s = pipeline.FeatureUnion([('raw_features', Get_Raw_Features()),\n",
    "                                               ('category_mean',Get_Category_ShrinkMean()),\n",
    "                                               ('text_features',Get_Text_Features()),\n",
    "                                               ('location_feature',Get_Location_Input()),\n",
    "                                              ])\n",
    "for model_name, model in model_dict.items():\n",
    "    print_model_result(model, model_name,combined_features_s,feature_name,X,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune hyperprameters for gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ({'gb__n_estimators': 50}, 0.70534596882581591)\n",
    "param_test1 = dict(gb__n_estimators = range(20,81,10))\n",
    "\n",
    "steps = [('combined',combined_features),\n",
    "         ('gb',GradientBoostingClassifier(learning_rate=0.2, min_samples_split=500,\\\n",
    "                                                            min_samples_leaf=50,max_depth=8,max_features='sqrt',\\\n",
    "                                                            subsample=0.8,random_state=10)),]\n",
    "cf1 = pipeline.Pipeline(steps)\n",
    "\n",
    "grid1= GridSearchCV(cf1,\\\n",
    "                    param_grid=param_test1, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid1.fit(X_train,y_train)\n",
    "#grid1.cv_results_, \\\n",
    "grid1.best_params_, \\\n",
    "grid1.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ({'gb__max_depth': 6, 'gb__min_samples_split': 800}, 0.70520058108538486)\n",
    "param_test2 = dict(gb__max_depth = range(3,7,1), gb__min_samples_split = range(200,1001,200))\n",
    "\n",
    "steps2 = [('combined',combined_features),\n",
    "         ('gb',GradientBoostingClassifier(n_estimators = 50, learning_rate=0.2, \\\n",
    "                                          max_features='sqrt',\\\n",
    "                                          subsample=0.8,random_state=10)),]\n",
    "cf2 = pipeline.Pipeline(steps2)\n",
    "\n",
    "grid2= GridSearchCV(cf2,\\\n",
    "                    param_grid=param_test2, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid2.fit(X_train,y_train)\n",
    "#grid1.cv_results_, \\\n",
    "grid2.best_params_, \\\n",
    "grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#({'gb__min_samples_leaf': 60, 'gb__min_samples_split': 2000}, 0.70525856924776154)\n",
    "\n",
    "param_test3 = dict(gb__min_samples_split = range(2000,4100,200), gb__min_samples_leaf = range(30,71,10))\n",
    "steps3 = [('combined',combined_features),\n",
    "         ('gb',GradientBoostingClassifier(n_estimators = 50, learning_rate=0.2, \\\n",
    "                                          max_depth = 6,\\\n",
    "                                          max_features='sqrt',\\\n",
    "                                          subsample=0.8,random_state=10)),]\n",
    "cf3 = pipeline.Pipeline(steps3)\n",
    "\n",
    "grid3= GridSearchCV(cf3,\\\n",
    "                    param_grid=param_test3, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid3.fit(X_train,y_train)\n",
    "\n",
    "grid3.best_params_, \\\n",
    "grid3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ({'gb__max_features': 13}, 0.70684376863401222)\n",
    "\n",
    "param_test4 = dict(gb__max_features = range(5,20,2))\n",
    "steps4 = [('combined',combined_features),\n",
    "         ('gb',GradientBoostingClassifier(n_estimators = 50, learning_rate=0.2, \\\n",
    "                                          max_depth = 6,min_samples_leaf=60,min_samples_split=2000, \\\n",
    "                                          subsample=0.8,random_state=10)),]\n",
    "cf4 = pipeline.Pipeline(steps4)\n",
    "\n",
    "grid4= GridSearchCV(cf4,\\\n",
    "                    param_grid=param_test4, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid4.fit(X_train,y_train)\n",
    "\n",
    "grid4.best_params_, \\\n",
    "grid4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ({'gb__subsample': 0.9}, 0.90704918296693149)\n",
    "\n",
    "param_test5 = dict(gb__subsample = [0.80,0.85,0.90,0.95])\n",
    "\n",
    "steps5 = [('combined',combined_features),\n",
    "         ('gb',GradientBoostingClassifier(n_estimators = 50, learning_rate=0.2, \\\n",
    "                                          max_depth = 6,min_samples_leaf=50,min_samples_split=2000, \\\n",
    "                                          random_state=10,max_features=13)),]\n",
    "cf5 = pipeline.Pipeline(steps5)\n",
    "\n",
    "grid5= GridSearchCV(cf5,\\\n",
    "                    param_grid=param_test5, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid5.fit(X_train,y_train)\n",
    "\n",
    "grid5.best_params_, \\\n",
    "grid5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps6 = [('combined',combined_features),\n",
    "         ('gb',GradientBoostingClassifier(n_estimators = 1000, learning_rate=0.01, \\\n",
    "                                          max_depth = 6,min_samples_leaf=50,min_samples_split=2000, \\\n",
    "                                          random_state=10,max_features=13, subsample=0.90)),]\n",
    "cf6 = pipeline.Pipeline(steps6)\n",
    "np.mean(cross_val_score(cf6, X_train, y_train, scoring = 'roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf6.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,cf6.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = cf6.named_steps['gb']\n",
    "feature_union_step = cf6.named_steps['combined']\n",
    "print_feature_importances(model,feature_union_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model,feature_union_step,index_list=[]):\n",
    "    if 'feature_importances_' in dir(model):\n",
    "        coefs = list(model.feature_importances_)\n",
    "    elif 'coef_' in dir(model):\n",
    "        coefs = list(model.coef_[0])\n",
    "    if coefs:\n",
    "        feature_names = []\n",
    "        for item in reg2.named_steps['combined'].transformer_list:\n",
    "            feature_names += item[1]._get_param_names()\n",
    "        if index_list:\n",
    "            result = []\n",
    "            for index,item in enumerate(feature_names):\n",
    "                if index in index_list:\n",
    "                    result.append(item)\n",
    "            feature_names = result\n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "\n",
    "        features = sorted(zip(feature_names,coefs),key=lambda x: abs(x[1]))\n",
    "        \n",
    "        s1 = pd.Series([f[0] for f in features], name='feature')\n",
    "        s2 = pd.Series([f[1] for f in features], name='importance')\n",
    "        pd.concat([s1,s2],axis=1).plot.barh(x='feature', y= 'importance',figsize=(20,15))\n",
    "        plt.savefig('feature_importance.png', fmt='png', dpi=300, bbox_inches='tight')\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         fig, ax = plt.barh(np.arange(len(features)), [f[1] for f in features])\n",
    "#         ax.set_yticks(np.arange(len(features)))\n",
    "#         ax.set_yticklabels([f[0] for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = cf6.named_steps['gb']\n",
    "feature_union_step = cf6.named_steps['combined']\n",
    "plot_feature_importances(model,feature_union_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune parameter selection for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_features = pipeline.FeatureUnion([('raw_features', Get_Raw_Features()),\n",
    "                                               ('category_mean',Get_Category_Mean()),\n",
    "                                               ('text_features',Get_Text_Features()),\n",
    "                                               ('location_feature',Get_Location_Input()),\n",
    "                                              ])\n",
    "\n",
    "param_lr1 = dict(selectk__k = range(5,32,2))\n",
    "\n",
    "step_lr = [('combined',combined_features),\n",
    "          ('selectk', SelectKBest()),\n",
    "         ('lr',LogisticRegression()),]\n",
    "lr2 = pipeline.Pipeline(step_lr)\n",
    "\n",
    "grid_lr = GridSearchCV(lr2,\\\n",
    "                    param_grid=param_lr1, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs = -1)\n",
    "\n",
    "\n",
    "grid_lr.fit(X_train,y_train)\n",
    "\n",
    "grid_lr.best_params_, \\\n",
    "grid_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model =  grid_lr.best_estimator_.named_steps['lr']\n",
    "feature_union_step = grid_lr.best_estimator_.named_steps['combined']\n",
    "\n",
    "print_feature_importances(model,feature_union_step,\\\n",
    "                         list(grid_lr.best_estimator_.named_steps['selectk'].get_support(indices=True))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune parameters for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#({'rf__min_samples_leaf': 60}, 0.69294110314893065)\n",
    "rf_param_test1 = dict(rf__min_samples_leaf = range(20,81,10))\n",
    "\n",
    "steps = [('combined',combined_features),\n",
    "         ('rf',RandomForestClassifier(n_estimators=10, criterion='entropy',\\\n",
    "                                      class_weight='balanced_subsample',random_state=10)),]\n",
    "rf_cf1 = pipeline.Pipeline(steps)\n",
    "\n",
    "rf_grid1= GridSearchCV(rf_cf1,\\\n",
    "                    param_grid=rf_param_test1, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf_grid1.fit(X_train,y_train)\n",
    "\n",
    "rf_grid1.best_params_, \\\n",
    "rf_grid1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ({'rf__max_features': 13}, 0.69574288015765484)\n",
    "\n",
    "rf_param_test2 = dict(rf__max_features = range(11,30,2))\n",
    "\n",
    "steps = [('combined',combined_features),\n",
    "         ('rf',RandomForestClassifier(n_estimators=10, criterion='entropy',\\\n",
    "                                      min_samples_leaf = 60,\\\n",
    "                                      class_weight='balanced_subsample',random_state=10)),]\n",
    "rf_cf2 = pipeline.Pipeline(steps)\n",
    "\n",
    "rf_grid2= GridSearchCV(rf_cf2,\\\n",
    "                    param_grid=rf_param_test2, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf_grid2.fit(X_train,y_train)\n",
    "\n",
    "rf_grid2.best_params_, \\\n",
    "rf_grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for time efficiency set n_estimators = 100\n",
    "rf_param_test3 = dict(rf__n_estimators = [10,50,100,200,400,600, 800])\n",
    "\n",
    "steps = [('combined',combined_features),\n",
    "         ('rf',RandomForestClassifier(criterion='entropy',\\\n",
    "                                      min_samples_leaf = 60,max_features=13,\\\n",
    "                                      class_weight='balanced_subsample',random_state=10)),]\n",
    "rf_cf3 = pipeline.Pipeline(steps)\n",
    "\n",
    "rf_grid3= GridSearchCV(rf_cf3,\\\n",
    "                    param_grid=rf_param_test3, \\\n",
    "                    scoring='roc_auc',\\\n",
    "                    n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf_grid3.fit(X_train,y_train)\n",
    "\n",
    "rf_grid3.best_params_, \\\n",
    "rf_grid3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps_rf = [('combined',combined_features),\n",
    "         ('rf',RandomForestClassifier(criterion='entropy', n_estimators = 100,\\\n",
    "                                      min_samples_leaf = 60,max_features=13,\\\n",
    "                                      class_weight='balanced_subsample',random_state=10)),]\n",
    "rf1 = pipeline.Pipeline(steps_rf)\n",
    "rf1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,rf1.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model =  rf1.named_steps['rf']\n",
    "feature_union_step = rf1.named_steps['combined']\n",
    "\n",
    "print_feature_importances(model,feature_union_step) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prec, recall, thresholds = precision_recall_curve(y_test,cf6.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "prec_lr, recall_lr, thresholds_lr = precision_recall_curve(y_test,grid_lr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_result = np.maximum(rf1.predict_proba(X_test)[:,1], cf6.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prec_en, recall_en, thresholds_en = precision_recall_curve(y_test,ensemble_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "prec_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test,rf1.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prec_d, recall_d, thresholds_d = precision_recall_curve(y_test,y_test_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prec_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(prec_rf[0:-1], recall_rf[0:-1], color='blue')\n",
    "\n",
    "plt.plot(prec[0:-1], recall[0:-1],'red')\n",
    "plt.plot(prec_lr[0:-1], recall_lr[0:-1], color = 'black')\n",
    "#plt.plot(prec_d[0:-1], recall_d[0:-1], color = 'green')\n",
    "\n",
    "plt.legend(['random forest','gradient boosting','logistic'],fontsize=22)\n",
    "\n",
    "plt.title('precision-recall curve',fontsize=22)\n",
    "plt.xlabel('precision',fontsize=22)\n",
    "plt.ylabel('recall',fontsize=22);\n",
    "plt.savefig('precision_recall.png',fmt='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# double check precision recall on holdout group\n",
    "prec_h, recall_h, thresholds = precision_recall_curve(y_holdout,cf6.predict_proba(X_holdout)[:,1])\n",
    "\n",
    "prec_lr_h, recall_lr_h, thresholds_lr = precision_recall_curve(y_holdout,grid_lr.predict_proba(X_holdout)[:,1])\n",
    "\n",
    "prec_rf_h, recall_rf_h, thresholds_rf_h = precision_recall_curve(y_holdout,rf1.predict_proba(X_holdout)[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prec_rf_h[0:-1], recall_rf_h[0:-1], color='blue')\n",
    "\n",
    "plt.plot(prec_h[0:-1], recall_h[0:-1],'orange')\n",
    "plt.plot(prec_lr_h[0:-1], recall_lr_h[0:-1], color = 'red')\n",
    "plt.legend(['random forest','gradient boosting','logistic'])\n",
    "\n",
    "\n",
    "plt.title('precision-recall curve for')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set threshold to optimize F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cf6.fit(X_train,y_train)\n",
    "f1_list = []\n",
    "y_pred_test_proba = cf6.predict_proba(X_test)[:,1]\n",
    "\n",
    "for threshold in np.arange(0.01,0.99,0.01):\n",
    "    y_test_pred = np.array(y_pred_test_proba>threshold).astype(int)\n",
    "    f1_list.append(f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(0.01,0.99,0.01)[np.argmax(f1_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, np.array(y_pred_test_proba>0.16).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, np.array(y_pred_test_proba>0.44).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_score(y_test, np.array(y_pred_test_proba>0.44).astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
